{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGAN_VDB.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"hGojyPlFSsem","colab_type":"code","outputId":"51010021-5df6-46b4-a505-6fe48435de17","executionInfo":{"status":"ok","timestamp":1556789290264,"user_tz":-540,"elapsed":705,"user":{"displayName":"綱島秀樹","photoUrl":"","userId":"10065336210090864101"}},"colab":{"base_uri":"https://localhost:8080/","height":298}},"cell_type":"code","source":["import chainer\n","chainer.print_runtime_info()\n","print('GPU availability:', chainer.cuda.available)\n","print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Platform: Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\n","Chainer: 5.4.0\n","NumPy: 1.16.3\n","CuPy:\n","  CuPy Version          : 5.4.0\n","  CUDA Root             : /usr/local/cuda\n","  CUDA Build Version    : 10000\n","  CUDA Driver Version   : 10000\n","  CUDA Runtime Version  : 10000\n","  cuDNN Build Version   : 7301\n","  cuDNN Version         : 7301\n","  NCCL Build Version    : 2402\n","  NCCL Runtime Version  : 2402\n","iDeep: 2.0.0.post3\n","GPU availability: True\n","cuDNN availablility: True\n"],"name":"stdout"}]},{"metadata":{"id":"3aSXQLszSsEV","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yXHVykMQVmIw","colab_type":"code","colab":{}},"cell_type":"code","source":["# Network_______________________________________________________________________\n","import numpy as np\n","\n","import chainer\n","import chainer.links as L\n","import chainer.distributions as D\n","from chainer import functions as F\n","from chainer import backend\n","\n","\n","def add_noise(h, sigma=0.2):\n","    xp = backend.get_array_module(h.array)\n","    if chainer.config.train:\n","        return h + sigma * xp.random.randn(*h.shape)\n","    else:\n","        return h\n","\n","\n","class Generator(chainer.Chain):\n","\n","    def __init__(self, n_hidden=128, bottom_width=4, ch=512, wscale=0.02):\n","        super(Generator, self).__init__()\n","        self.n_hidden = n_hidden\n","        self.ch = ch\n","        self.bottom_width = bottom_width\n","\n","        with self.init_scope():\n","            w = chainer.initializers.Normal(wscale)\n","            self.l0 = L.Linear(self.n_hidden, bottom_width * bottom_width * ch,\n","                               initialW=w)\n","            self.c1 = L.Convolution2D(ch, ch // 2, 3, 1, 1, initialW=w)\n","            self.c2 = L.Convolution2D(ch // 2, ch // 4, 3, 1, 1, initialW=w)\n","            self.c3 = L.Convolution2D(ch // 4, ch // 8, 3, 1, 1, initialW=w)\n","            self.c4 = L.Convolution2D(ch // 8, 3, 3, 1, 1, initialW=w)\n","            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)\n","            self.bn1 = L.BatchNormalization(ch // 2)\n","            self.bn2 = L.BatchNormalization(ch // 4)\n","            self.bn3 = L.BatchNormalization(ch // 8)\n","\n","    def make_hidden(self, batchsize):\n","        dtype = chainer.get_dtype()\n","        return np.random.uniform(-1, 1, (batchsize, self.n_hidden, 1, 1))\\\n","            .astype(dtype)\n","\n","    def forward(self, z):\n","        h = F.reshape(F.relu(self.bn0(self.l0(z))),\n","                      (len(z), self.ch, self.bottom_width, self.bottom_width))\n","        h = F.unpooling_2d(h, ksize=2, outsize=(8,8)) # 8x8\n","        h = F.relu(self.bn1(self.c1(h)))\n","        h = F.unpooling_2d(h, ksize=2, outsize=(16,16)) # 16x16\n","        h = F.relu(self.bn2(self.c2(h)))\n","        h = F.unpooling_2d(h, ksize=2, outsize=(32,32)) # 32x32\n","        h = F.relu(self.bn3(self.c3(h)))\n","        x = F.sigmoid(self.c4(h))\n","        return x\n","\n","    \n","# Discriminator with Variational Discriminator Bottleneck\n","class Discriminator(chainer.Chain):\n","\n","    def __init__(self, n_latent=256, ch=256, bottom_width=4, wscale=0.02, k=1):\n","        self.k = k\n","        super(Discriminator, self).__init__()\n","        with self.init_scope():\n","            w = chainer.initializers.Normal(wscale)\n","            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)\n","            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)\n","            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)\n","            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)\n","            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)\n","            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)\n","            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)\n","            self.mu = L.Convolution2D(ch // 1, n_latent, 1, 1, 0, initialW=w)\n","            self.ln_sigma = L.Convolution2D(ch // 1, n_latent, 1, 1, 0, initialW=w)\n","            self.out = L.Linear(n_latent * bottom_width**2, 1, initialW=w)\n","            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)\n","            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)\n","            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)\n","            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)\n","            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)\n","            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)\n","\n","    def forward(self, x, gen=False):\n","        h = F.leaky_relu(self.c0_0(x))\n","        h = F.leaky_relu(self.bn0_1(self.c0_1(h)))\n","        h = F.leaky_relu(self.bn1_0(self.c1_0(h)))\n","        h = F.leaky_relu(self.bn1_1(self.c1_1(h)))\n","        h = F.leaky_relu(self.bn2_0(self.c2_0(h)))\n","        h = F.leaky_relu(self.bn2_1(self.c2_1(h)))\n","        h = F.leaky_relu(self.bn3_0(self.c3_0(h)))\n","        mu = self.mu(h)\n","        ln_sigma = self.ln_sigma(h)  # log(sigma)\n","        \n","        dist = D.Normal(loc=mu, log_scale=ln_sigma)\n","        if gen:\n","            out = F.sigmoid(self.out(mu))\n","        else:\n","            z = dist.sample(self.k)\n","            z = F.sum(z, axis=0) / self.k\n","            out = F.sigmoid(self.out(z))\n","        return out, dist"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1T-bsjVtgrxu","colab_type":"code","colab":{}},"cell_type":"code","source":["# Updater_______________________________________________________________________\n","import numpy as np\n","\n","import chainer\n","import chainer.functions as F\n","from chainer import Variable\n","\n","# Classic Adversarial Loss\n","def loss_dcgan_dis(dis_fake, dis_real):\n","    L1 = F.mean(F.softplus(-dis_real))\n","    L2 = F.mean(F.softplus(dis_fake))\n","    loss = L1 + L2\n","    return loss\n","\n","\n","def loss_dcgan_gen(dis_fake):\n","    loss = F.mean(F.softplus(-dis_fake))\n","    return loss\n","\n","\n","# Hinge Loss\n","def loss_hinge_dis(dis_fake, dis_real):\n","    loss = F.mean(F.relu(1. - dis_real))\n","    loss += F.mean(F.relu(1. + dis_fake))\n","    return loss\n","\n","\n","def loss_hinge_gen(dis_fake):\n","    loss = -F.mean(dis_fake)\n","    return loss\n","\n","\n","# VGAN Loss\n","class Prior(chainer.Link):\n","\n","    def __init__(self, n_latent=256, bottom_width=4, batchsize=None):\n","        super(Prior, self).__init__()\n","\n","        self.loc = np.zeros(\n","            (batchsize, n_latent, bottom_width, bottom_width), np.float32)\n","        self.scale = np.ones(\n","            (batchsize, n_latent, bottom_width, bottom_width), np.float32)\n","        self.register_persistent('loc')\n","        self.register_persistent('scale')\n","\n","    def forward(self):\n","        return D.Normal(self.loc, scale=self.scale)\n","\n","    \n","def loss_vgan_dis(dis_fake, dis_real, latent_gx, latent_px, r_z, Ic=0.1, beta=1.0):\n","    L1 = F.mean(F.softplus(-dis_real))\n","    L2 = F.mean(F.softplus(dis_fake))\n","    kl_gx = F.mean(F.sum(chainer.kl_divergence(latent_gx, r_z), axis=-1))\n","    kl_px = F.mean(F.sum(chainer.kl_divergence(latent_px, r_z), axis=-1))\n","    kl = (kl_gx + kl_px) * 0.5\n","    L3 = (kl - Ic) * beta\n","    loss = L1 + L2 + L3\n","    return loss, kl\n","\n","\n","def loss_vgan_gen(dis_fake):\n","    loss = F.mean(F.softplus(-dis_fake))\n","    return loss\n","\n","\n","class Updater(chainer.training.StandardUpdater):\n","    def __init__(self, *args, **kwargs):\n","        self.models = kwargs.pop('models')\n","        self.n_dis = kwargs.pop('n_dis')\n","        self.beta= kwargs.pop(\"beta\")\n","        self.beta_step = kwargs.pop(\"beta_step\")\n","        self.Ic = kwargs.pop(\"Ic\")\n","        self.k = kwargs.pop(\"k\")\n","        self.loss_type = kwargs.pop('loss_type')\n","        if self.loss_type == 'dcgan':\n","            self.loss_gen = loss_dcgan_gen\n","            self.loss_dis = loss_dcgan_dis\n","        elif self.loss_type == 'hinge':\n","            self.loss_gen = loss_hinge_gen\n","            self.loss_dis = loss_hinge_dis\n","        elif self.loss_type == \"vgan\":\n","            self.loss_gen = loss_vgan_gen\n","            self.loss_dis = loss_vgan_dis\n","        else:\n","            raise NotImplementedError\n","        super(Updater, self).__init__(*args, **kwargs)\n","        \n","    def update_beta(self, avg_kl):\n","        with chainer.using_config(\"train\", False), chainer.using_config(\"enable_backprop\", False):\n","            new_beta = self.beta - self.beta_step * (self.Ic - avg_kl)\n","            new_beta = max(new_beta.data, 0)\n","            self.beta = Variable(new_beta)\n","        \n","    def update_core(self):\n","        gen = self.models['gen']\n","        dis = self.models['dis']\n","        prior = self.models[\"prior\"]\n","        gen_optimizer = self.get_optimizer('opt_gen')\n","        dis_optimizer = self.get_optimizer('opt_dis')\n","        xp = gen.xp\n","        for i in range(self.n_dis):\n","            x_real = self.get_iterator(\"main\").next()\n","            x_real = Variable(self.converter(x_real, self.device)) / 255.\n","            batchsize = len(x_real)\n","            if i == 0:\n","                z = Variable(xp.asarray(gen.make_hidden(batchsize)))\n","                x_fake = gen(z)\n","                dis_fake, _ = dis(x_fake, gen=True)\n","                loss_gen = self.loss_gen(dis_fake)\n","                gen.cleargrads()\n","                loss_gen.backward()\n","                gen_optimizer.update()\n","                chainer.reporter.report({'loss_gen': loss_gen})\n","            \n","            z = Variable(xp.asarray(gen.make_hidden(batchsize)))\n","            x_fake = gen(z)\n","            dis_fake, latent_gx = dis(x_fake, gen=True)\n","            dis_real, latent_px = dis(x_real)\n","            r_z = prior()\n","            x_fake.unchain_backward()\n","\n","            loss_dis, avg_kl = self.loss_dis(\n","                dis_fake, dis_real, latent_gx, latent_px, r_z, self.Ic, self.beta)\n","            dis.cleargrads()\n","            loss_dis.backward()\n","            dis_optimizer.update()\n","            chainer.reporter.report({'loss_dis': loss_dis})\n","            \n","            self.update_beta(avg_kl) # Updating beta"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OyhrWiq4wHQU","colab_type":"text"},"cell_type":"markdown","source":["# Please change variable \"out\" to your directory in which you want to save several snapshots."]},{"metadata":{"id":"Lt6WTspSiT6M","colab_type":"code","outputId":"e1fb940e-6235-49e1-8eef-50d4c458bafa","executionInfo":{"status":"error","timestamp":1556789315803,"user_tz":-540,"elapsed":1627,"user":{"displayName":"綱島秀樹","photoUrl":"","userId":"10065336210090864101"}},"colab":{"base_uri":"https://localhost:8080/","height":354}},"cell_type":"code","source":["# Train_________________________________________________________________________\n","import os\n","\n","import chainer\n","from chainer import training\n","from chainer.training import extension\n","from chainer.training import extensions\n","from chainer.datasets import split_dataset\n","from chainer import serializers\n","\n","\n","def make_optimizer(model, alpha=0.0002, beta1=0., beta2=0.9):\n","    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1, beta2=beta2)\n","    optimizer.setup(model)\n","    return optimizer\n","\n","def main():\n","\n","    # Prepare parameter1\n","    batchsize = 256\n","    iteration = int((40000 * 200) / batchsize)\n","    gpu = 0\n","    dim_z = 256\n","    n_dis = 1\n","    AdamAlpha = 0.0002\n","    beta = 0.1\n","    beta_step = 0.00001\n","    Ic = 0.1\n","    k = 1 # If the \"batchsize\" is lower than 128, \"k\" must be set more than 1.\n","    loss_type = \"vgan\" # dcgan or hinge or vgan\n","    out = \"\" \n","    resume = \"\"\n","    seed = 0\n","    display_interval = 10\n","    snapshot_interval = 40 # snapshot per 40 epochs\n","    iteration_decay_start = 0\n","    iteration_decay_end = iteration\n","    \n","    # Dataset\n","    train, test = chainer.datasets.get_cifar10(withlabel=False, scale=255.)\n","    train, valid = split_dataset(train, int(len(train) * 0.8))\n","    print('# data-size: {}'.format(len(train)))\n","    print('# data-shape: {}'.format(train[0][0].shape))\n","    print('')\n","    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n","    valid_iter = chainer.iterators.SerialIterator(valid, batchsize, False, False)\n","    \n","    print('GPU: {}'.format(gpu))\n","    print('# Minibatch-size: {}'.format(batchsize))\n","    print('# epoch: {}'.format(int((batchsize*iteration)/len(train))))\n","    \n","    # Set up a models\n","    gen = Generator()\n","    dis = Discriminator(n_latent=dim_z, k=k)\n","    prior = Prior(dim_z, batchsize=batchsize)\n","    \n","    if gpu >= 0:\n","        # Make a specified GPU current\n","        chainer.cuda.get_device_from_id(gpu).use()\n","        gen.to_gpu() # Copy the model to the GPU\n","        dis.to_gpu()\n","        prior.to_gpu()\n","        \n","    # Optimizer\n","    opt_gen = make_optimizer(gen)\n","    opt_dis = make_optimizer(dis)\n","    \n","    # Trainer\n","    updater = Updater(\n","        models={\"gen\": gen, \"dis\": dis, \"prior\": prior},\n","        n_dis=n_dis,\n","        beta=beta,\n","        beta_step=beta_step,\n","        Ic=Ic,\n","        k=k,\n","        loss_type=loss_type,\n","        iterator=train_iter,\n","        optimizer={\"opt_gen\": opt_gen, \"opt_dis\": opt_dis},\n","        device=gpu)\n","    trainer = training.Trainer(updater, (iteration, \"iteration\"), out=out)\n","    \n","    # Set up logging\n","    snapshot_interval = (snapshot_interval, \"epoch\")\n","    display_interval = (display_interval, \"iteration\")\n","    trainer.extend(extensions.snapshot_object(gen, 'gen_epoch_{.updater.epoch}.npz'), trigger=snapshot_interval)\n","    trainer.extend(extensions.snapshot_object(dis, 'dis_epoch_{.updater.epoch}.npz'), trigger=snapshot_interval)\n","    trainer.extend(extensions.LogReport(trigger=display_interval))\n","    trainer.extend(extensions.PrintReport(['epoch', 'iteration', 'loss_gen', \"loss_dis\", \"elapsed_time\"]), trigger=display_interval)\n","    trainer.extend(extensions.PlotReport( ['loss_gen', \"loss_dis\"], x_key='iteration', trigger=display_interval, file_name=\"plot.png\"))\n","    trainer.extend(extensions.ProgressBar(update_interval=10))\n","    ext_opt_gen = extensions.LinearShift('alpha', (AdamAlpha, 0.),\n","                                         (iteration_decay_start, iteration_decay_end), opt_gen)\n","    ext_opt_dis = extensions.LinearShift('alpha', (AdamAlpha, 0.),\n","                                         (iteration_decay_start, iteration_decay_end), opt_dis)\n","    trainer.extend(ext_opt_gen)\n","    trainer.extend(ext_opt_dis)\n","    \n","    if resume:\n","        # Resume from a snapshot\n","        chainer.serializers.load_npz(resume, trainer)\n","\n","    # Run the training\n","    print(\"start training\")\n","    trainer.run()\n","\n","    # Save decoder and discriminator models\n","    if gpu >= 0:\n","        gen.to_cpu()\n","        dis.to_cpu()\n","    chainer.serializers.save_npz(os.path.join(out, 'gen.npz'), gen)\n","    chainer.serializers.save_npz(os.path.join(out, \"dis.npz\"), dis)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-a660dd0024be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-a660dd0024be>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'# data-size: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/datasets/cifar.py\u001b[0m in \u001b[0;36mget_cifar10\u001b[0;34m(withlabel, ndim, scale, dtype)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_cifar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar-10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/datasets/cifar.py\u001b[0m in \u001b[0;36m_get_cifar\u001b[0;34m(name, withlabel, ndim, scale, dtype)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_or_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     train = _preprocess_cifar(raw['train_x'], raw['train_y'], withlabel,\n\u001b[0m\u001b[1;32m    138\u001b[0m                               ndim, scale, dtype)\n\u001b[1;32m    139\u001b[0m     test = _preprocess_cifar(raw['test_x'], raw['test_y'], withlabel, ndim,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    732\u001b[0m                                                              count=read_count)\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m             self._eof = (self._decompressor.eof or\n\u001b[1;32m    950\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"0WM9UnrfSPN9","colab_type":"code","colab":{}},"cell_type":"code","source":["from chainer import serializers\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from chainer.cuda import to_cpu\n","from chainer import Variable\n","\n","\n","# Prepare parameter\n","gpu = 0\n","batchsize = 1\n","    \n","# Make Encoder and decoder models\n","inf_gen = Generator()\n","\n","# Load model parameter\n","serializers.load_npz(\n","    \"drive/My Drive/Google_Colab/DCGAN_VDB/results/DCGAN_VDB_nonAdaptive/gen.npz\", inf_gen) # Please fill out the snapshot path into \"\".\n","\n","inf_gen.to_gpu(gpu)\n","\n","with chainer.using_config(\"train\", False), chainer.using_config(\"enable_backprop\", False):\n","    inf_z = Variable(inf_gen.xp.asarray(inf_gen.make_hidden(batchsize)))\n","    inf_x = inf_gen(z=inf_z)\n","    \n","inf_x = to_cpu(inf_x.array)\n","inf_x = (inf_x + 1) / 2 # If you use tanh in the output of the generator, you must be enable this row. \n","inf_x = inf_x * 255\n","inf_x_show = np.zeros((3, 32, 32))\n","for i in range(3):\n","    for j in range(32):\n","        for k in range(32):\n","            inf_x_show[i][j][k] = inf_x[0][i][j][k]\n","            \n","inf_x_show = inf_x_show.transpose(1, 2, 0)\n","pil_img = Image.fromarray(np.uint8(inf_x_show))\n","plt.imshow(pil_img)"],"execution_count":0,"outputs":[]}]}