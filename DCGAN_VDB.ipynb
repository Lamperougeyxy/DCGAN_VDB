{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGAN_VDB.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"hGojyPlFSsem","colab_type":"code","colab":{}},"cell_type":"code","source":["import chainer\n","chainer.print_runtime_info()\n","print('GPU availability:', chainer.cuda.available)\n","print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3aSXQLszSsEV","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yXHVykMQVmIw","colab_type":"code","colab":{}},"cell_type":"code","source":["# Network_______________________________________________________________________\n","import numpy as np\n","\n","import chainer\n","import chainer.links as L\n","import chainer.distributions as D\n","from chainer import functions as F\n","from chainer import backend\n","\n","\n","def add_noise(h, sigma=0.2):\n","    xp = backend.get_array_module(h.array)\n","    if chainer.config.train:\n","        return h + sigma * xp.random.randn(*h.shape)\n","    else:\n","        return h\n","\n","\n","class Generator(chainer.Chain):\n","\n","    def __init__(self, n_hidden=128, bottom_width=4, ch=512, wscale=0.02):\n","        super(Generator, self).__init__()\n","        self.n_hidden = n_hidden\n","        self.ch = ch\n","        self.bottom_width = bottom_width\n","\n","        with self.init_scope():\n","            w = chainer.initializers.Normal(wscale)\n","            self.l0 = L.Linear(self.n_hidden, bottom_width * bottom_width * ch,\n","                               initialW=w)\n","            self.c1 = L.Convolution2D(ch, ch // 2, 3, 1, 1, initialW=w)\n","            self.c2 = L.Convolution2D(ch // 2, ch // 4, 3, 1, 1, initialW=w)\n","            self.c3 = L.Convolution2D(ch // 4, ch // 8, 3, 1, 1, initialW=w)\n","            self.c4 = L.Convolution2D(ch // 8, 3, 3, 1, 1, initialW=w)\n","            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)\n","            self.bn1 = L.BatchNormalization(ch // 2)\n","            self.bn2 = L.BatchNormalization(ch // 4)\n","            self.bn3 = L.BatchNormalization(ch // 8)\n","\n","    def make_hidden(self, batchsize):\n","        dtype = chainer.get_dtype()\n","        return np.random.uniform(-1, 1, (batchsize, self.n_hidden, 1, 1))\\\n","            .astype(dtype)\n","\n","    def forward(self, z):\n","        h = F.reshape(F.relu(self.bn0(self.l0(z))),\n","                      (len(z), self.ch, self.bottom_width, self.bottom_width))\n","        h = F.unpooling_2d(h, ksize=2, outsize=(8,8)) # 8x8\n","        h = F.relu(self.bn1(self.c1(h)))\n","        h = F.unpooling_2d(h, ksize=2, outsize=(16,16)) # 16x16\n","        h = F.relu(self.bn2(self.c2(h)))\n","        h = F.unpooling_2d(h, ksize=2, outsize=(32,32)) # 32x32\n","        h = F.relu(self.bn3(self.c3(h)))\n","        x = F.sigmoid(self.c4(h))\n","        return x\n","\n","\n","class Discriminator(chainer.Chain):\n","\n","    def __init__(self, wscale=0.02):\n","        w = chainer.initializers.Normal(wscale)\n","        super(Discriminator, self).__init__()\n","        with self.init_scope():\n","            self.l0 = L.Linear(None, 1024, initialW=w)\n","            self.l1 = L.Linear(1024, 1024, initialW=w)\n","            self.l2 = L.Linear(1024, 1, initialW=w)\n","\n","    def forward(self, x):\n","        h = F.leaky_relu(self.l0(x))\n","        h = F.leaky_relu(self.l1(h))\n","        return self.l2(h)\n","    \n","    \n","# Variational Discriminator Bottleneck\n","class Encoder(chainer.Chain):\n","\n","    def __init__(self, n_latent, ch=256, bottom_width=4, wscale=0.02):\n","        super(Encoder, self).__init__()\n","        with self.init_scope():\n","            w = chainer.initializers.Normal(wscale)\n","            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)\n","            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)\n","            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)\n","            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)\n","            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)\n","            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)\n","            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)\n","            self.mu = L.Linear(\n","                 bottom_width * bottom_width * ch, n_latent, initialW=w)\n","            self.ln_sigma = L.Linear(\n","                 bottom_width * bottom_width * ch, n_latent, initialW=w)\n","            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)\n","            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)\n","            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)\n","            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)\n","            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)\n","            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)\n","\n","    def forward(self, x):\n","        h = F.leaky_relu(self.c0_0(x))\n","        h = F.leaky_relu(self.bn0_1(self.c0_1(h)))\n","        h = F.leaky_relu(self.bn1_0(self.c1_0(h)))\n","        h = F.leaky_relu(self.bn1_1(self.c1_1(h)))\n","        h = F.leaky_relu(self.bn2_0(self.c2_0(h)))\n","        h = F.leaky_relu(self.bn2_1(self.c2_1(h)))\n","        h = F.leaky_relu(self.bn3_0(self.c3_0(h)))\n","        mu = self.mu(h)\n","        ln_sigma = self.ln_sigma(h)  # log(sigma)\n","        return D.Normal(loc=mu, log_scale=ln_sigma), mu"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1T-bsjVtgrxu","colab_type":"code","colab":{}},"cell_type":"code","source":["# Updater_______________________________________________________________________\n","import numpy as np\n","\n","import chainer\n","import chainer.functions as F\n","from chainer import Variable\n","\n","# Classic Adversarial Loss\n","def loss_dcgan_dis(dis_fake, dis_real):\n","    L1 = F.mean(F.softplus(-dis_real))\n","    L2 = F.mean(F.softplus(dis_fake))\n","    loss = L1 + L2\n","    return loss\n","\n","\n","def loss_dcgan_gen(dis_fake):\n","    loss = F.mean(F.softplus(-dis_fake))\n","    return loss\n","\n","\n","# Hinge Loss\n","def loss_hinge_dis(dis_fake, dis_real):\n","    loss = F.mean(F.relu(1. - dis_real))\n","    loss += F.mean(F.relu(1. + dis_fake))\n","    return loss\n","\n","\n","def loss_hinge_gen(dis_fake):\n","    loss = -F.mean(dis_fake)\n","    return loss\n","\n","\n","# VGAN Loss\n","class Prior(chainer.Link):\n","\n","    def __init__(self, n_latent):\n","        super(Prior, self).__init__()\n","\n","        self.loc = np.zeros(n_latent, np.float32)\n","        self.scale = np.ones(n_latent, np.float32)\n","        self.register_persistent('loc')\n","        self.register_persistent('scale')\n","\n","    def forward(self):\n","        return D.Normal(self.loc, scale=self.scale)\n","\n","    \n","def loss_vgan_dis(fake_z, real_z, latent, r_z, Ic=0.1, beta=1.0):\n","    L1 = F.mean(F.softplus(-real_z))\n","    L2 = F.mean(F.softplus(fake_z))\n","    L3 = (F.mean(F.sum(chainer.kl_divergence(latent, r_z), axis=-1)) - Ic) * beta\n","    loss = L1 + L2 + L3\n","    return loss\n","\n","\n","def loss_vgan_gen(fake_mu):\n","    loss = F.mean(F.softplus(-fake_mu))\n","    return loss\n","     \n","    \n","def loss_enc(latent, r_z, Ic=0.1):\n","    loss = F.mean(F.sum(chainer.kl_divergence(latent, r_z), axis=-1)) - Ic\n","    return loss\n","\n","\n","class Updater(chainer.training.StandardUpdater):\n","    def __init__(self, *args, **kwargs):\n","        self.models = kwargs.pop('models')\n","        self.n_dis = kwargs.pop('n_dis')\n","        self.beta= kwargs.pop(\"beta\")\n","        self.Ic = kwargs.pop(\"Ic\")\n","        self.k = kwargs.pop(\"k\")\n","        self.loss_type = kwargs.pop('loss_type')\n","        if self.loss_type == 'dcgan':\n","            self.loss_gen = loss_dcgan_gen\n","            self.loss_dis = loss_dcgan_dis\n","        elif self.loss_type == 'hinge':\n","            self.loss_gen = loss_hinge_gen\n","            self.loss_dis = loss_hinge_dis\n","        elif self.loss_type == \"vgan\":\n","            self.loss_gen = loss_vgan_gen\n","            self.loss_dis = loss_vgan_dis\n","            self.loss_enc = loss_enc\n","        else:\n","            raise NotImplementedError\n","        super(Updater, self).__init__(*args, **kwargs)\n","        \n","    def update_core(self):\n","        gen = self.models['gen']\n","        dis = self.models['dis']\n","        enc = self.models[\"enc\"]\n","        prior = self.models[\"prior\"]\n","        gen_optimizer = self.get_optimizer('opt_gen')\n","        dis_optimizer = self.get_optimizer('opt_dis')\n","        enc_optimizer = self.get_optimizer(\"opt_enc\")\n","        xp = gen.xp\n","        for i in range(self.n_dis):\n","            x_real = self.get_iterator(\"main\").next()\n","            x_real = Variable(self.converter(x_real, self.device)) / 255.\n","            batchsize = len(x_real)\n","            if i == 0:\n","                z = Variable(xp.asarray(gen.make_hidden(batchsize)))\n","                x_fake = gen(z)\n","                _, mu = enc(x_fake)\n","                mu = F.reshape(mu, (1, -1))\n","                dis_fake_mu = dis(mu)\n","                loss_gen = self.loss_gen(dis_fake_mu)\n","                gen.cleargrads()\n","                loss_gen.backward()\n","                gen_optimizer.update()\n","                chainer.reporter.report({'loss_gen': loss_gen})\n","            \n","            z = Variable(xp.asarray(gen.make_hidden(batchsize)))\n","            x_fake = gen(z)\n","            x_tilde = F.concat((x_fake, x_real), axis=0)\n","\n","            latent, _ = enc(x_tilde)\n","            latent_px, _ = enc(x_real)\n","            latent_gx, _ = enc(x_fake)\n","            \n","            r_z = prior()\n","            z_px = latent_px.sample(self.k)\n","            z_gx = latent_gx.sample(self.k)\n","            \n","            dis_real = dis(z_px)\n","            dis_fake = dis(z_gx)\n","            x_fake.unchain_backward()\n","\n","            loss_dis = self.loss_dis(\n","                dis_fake, dis_real, latent, r_z, self.Ic, self.beta)\n","            dis.cleargrads()\n","            loss_dis.backward()\n","            dis_optimizer.update()\n","            chainer.reporter.report({'loss_dis': loss_dis})\n","            \n","            # Updating encoder\n","            loss_enc = self.loss_enc(latent=latent, r_z=r_z, Ic=self.Ic)\n","            enc.cleargrads()\n","            loss_enc.backward()\n","            enc_optimizer.update()\n","            chainer.reporter.report({'loss_enc': loss_enc})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OyhrWiq4wHQU","colab_type":"text"},"cell_type":"markdown","source":["# Please change variable \"out\" to your directory that you want to save several snapshots."]},{"metadata":{"id":"Lt6WTspSiT6M","colab_type":"code","colab":{}},"cell_type":"code","source":["# Train_________________________________________________________________________\n","import os\n","\n","import chainer\n","from chainer import training\n","from chainer.training import extension\n","from chainer.training import extensions\n","from chainer.datasets import split_dataset\n","from chainer import serializers\n","\n","\n","def make_optimizer(model, alpha=0.0002, beta1=0., beta2=0.9):\n","    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1, beta2=beta2)\n","    optimizer.setup(model)\n","    return optimizer\n","\n","def main():\n","\n","    # Prepare parameter1\n","    batchsize = 4\n","    iteration = 31250\n","    gpu = 0\n","    dim_z = 128\n","    n_dis = 1\n","    AdamAlpha = 0.0002\n","    beta = 0.1\n","    Ic = 0.1\n","    k = 1 # If the \"batchsize\" is lower than 128, \"k\" must be set more than 1.\n","    loss_type = \"vgan\" # dcgan or hinge or vgan\n","    out = \"\" \n","    resume = \"\"\n","    seed = 0\n","    \n","    # Dataset\n","    train, test = chainer.datasets.get_cifar10(withlabel=False, scale=255.)\n","    train, valid = split_dataset(train, int(len(train) * 0.8))\n","    print('# data-size: {}'.format(len(train)))\n","    print('# data-shape: {}'.format(train[0][0].shape))\n","    print('')\n","    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n","    valid_iter = chainer.iterators.SerialIterator(valid, batchsize, False, False)\n","    \n","    # Prepare parameter2\n","    display_interval = 10\n","    snapshot_interval = (len(train)/batchsize) * 20 # snapshot per 20 epochs\n","    iteration_decay_start = 0\n","    iteration_decay_end = iteration\n","    \n","    print('GPU: {}'.format(gpu))\n","    print('# Minibatch-size: {}'.format(batchsize))\n","    print('# epoch: {}'.format(int((batchsize*iteration)/len(train))))\n","    \n","    # Set up a models\n","    gen = Generator()\n","    dis = Discriminator()\n","    enc = Encoder(dim_z)\n","    prior = Prior(dim_z)\n","    \n","    if gpu >= 0:\n","        # Make a specified GPU current\n","        chainer.cuda.get_device_from_id(gpu).use()\n","        gen.to_gpu() # Copy the model to the GPU\n","        dis.to_gpu()\n","        enc.to_gpu()\n","        prior.to_gpu()\n","        \n","    # Optimizer\n","    opt_gen = make_optimizer(gen)\n","    opt_dis = make_optimizer(dis)\n","    opt_enc = make_optimizer(enc)\n","    \n","    # Trainer\n","    updater = Updater(\n","        models={\"gen\": gen, \"dis\": dis, \"enc\": enc, \"prior\": prior},\n","        n_dis=n_dis,\n","        beta=beta,\n","        Ic=Ic,\n","        k=k,\n","        loss_type=loss_type,\n","        iterator=train_iter,\n","        optimizer={\"opt_gen\": opt_gen, \"opt_dis\": opt_dis, \"opt_enc\": opt_enc},\n","        device=gpu)\n","    trainer = training.Trainer(updater, (iteration, \"iteration\"), out=out)\n","    \n","    # Set up logging\n","    snapshot_interval = (snapshot_interval, \"iteration\")\n","    display_interval = (display_interval, \"iteration\")\n","    trainer.extend(extensions.snapshot_object(gen, 'gen_epoch_{.updater.epoch}.npz'), trigger=snapshot_interval)\n","    trainer.extend(extensions.snapshot_object(dis, 'dis_epoch_{.updater.epoch}.npz'), trigger=snapshot_interval)\n","    trainer.extend(extensions.snapshot_object(enc, 'enc_epoch_{.updater.epoch}.npz'), trigger=snapshot_interval)\n","    trainer.extend(extensions.LogReport(trigger=display_interval))\n","    trainer.extend(extensions.PrintReport(['epoch', 'iteration', 'loss_gen', \"loss_dis\", \"loss_enc\", \"elapsed_time\"]), trigger=display_interval)\n","    trainer.extend(extensions.PlotReport( ['loss_gen', \"loss_dis\"], x_key='iteration', trigger=display_interval, file_name=\"plot.png\"))\n","    trainer.extend(extensions.PlotReport( ['loss_enc'], x_key='iteration', trigger=display_interval, file_name=\"enc_plot.png\"))\n","    trainer.extend(extensions.ProgressBar(update_interval=10))\n","    ext_opt_gen = extensions.LinearShift('alpha', (AdamAlpha, 0.),\n","                                         (iteration_decay_start, iteration_decay_end), opt_gen)\n","    ext_opt_dis = extensions.LinearShift('alpha', (AdamAlpha, 0.),\n","                                         (iteration_decay_start, iteration_decay_end), opt_dis)\n","    ext_opt_enc = extensions.LinearShift('alpha', (AdamAlpha, 0.),\n","                                         (iteration_decay_start, iteration_decay_end), opt_enc)\n","    trainer.extend(ext_opt_gen)\n","    trainer.extend(ext_opt_dis)\n","    trainer.extend(ext_opt_enc)\n","    \n","    if resume:\n","        # Resume from a snapshot\n","        chainer.serializers.load_npz(resume, trainer)\n","\n","    # Run the training\n","    print(\"start training\")\n","    trainer.run()\n","\n","    # Save decoder and discriminator models\n","    if gpu >= 0:\n","        gen.to_cpu()\n","        dis.to_cpu()\n","        enc.to_cpu()\n","    chainer.serializers.save_npz(os.path.join(out, 'gen.npz'), gen)\n","    chainer.serializers.save_npz(os.path.join(out, \"dis.npz\"), dis)\n","    chainer.serializers.save_npz(os.path.join(out, \"enc.npz\"), enc)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0WM9UnrfSPN9","colab_type":"code","colab":{}},"cell_type":"code","source":["from chainer import serializers\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from chainer.cuda import to_cpu\n","from chainer import Variable\n","\n","\n","# Prepare parameter\n","gpu = 0\n","batchsize = 1\n","    \n","# Make Encoder and decoder models\n","inf_gen = tGenerator()\n","\n","# Load model parameter\n","serializers.load_npz(\"\", inf_gen) # Please fill out the snapshot path into \"\".\n","\n","inf_gen.to_gpu(gpu)\n","\n","with chainer.using_config(\"train\", False), chainer.using_config(\"enable_backprop\", False):\n","    inf_z = sample_continuous(128, 1, distribution=\"normal\", xp=inf_gen.xp)\n","    inf_x = inf_gen(z=inf_z)\n","    \n","inf_x = to_cpu(inf_x.array)\n","inf_x = (inf_x + 1) / 2 # If you use tanh in the output of the generator, you must be enable this row. \n","inf_x = inf_x * 255\n","inf_x_show = np.zeros((3, 32, 32))\n","for i in range(3):\n","    for j in range(32):\n","        for k in range(32):\n","            inf_x_show[i][j][k] = inf_x[0][i][j][k]\n","            \n","inf_x_show = inf_x_show.transpose(1, 2, 0)\n","pil_img = Image.fromarray(np.uint8(inf_x_show))\n","plt.imshow(pil_img)"],"execution_count":0,"outputs":[]}]}